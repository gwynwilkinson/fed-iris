{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Tensorboard library was not found.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdp import PrivacyEngine\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 50 # About 250 is optimal for adult dataset\n",
    "        self.iterations = 1\n",
    "        self.lr = 0.01\n",
    "        self.shuffle_dataset = True\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "        self.disable_federated = True\n",
    "        self.disable_dp = True\n",
    "        self.alphas=[1.1, 2, 4, 10]\n",
    "        self.delta = 1.0\n",
    "        self.workers = 5\n",
    "        self.disable_verbose_training = True\n",
    "        self.no_cuda = True\n",
    "    \n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult(args):\n",
    "\n",
    "    data_file = \"/home/gwyn/dev/webapp/test/fed-iris/data/adult.data\"\n",
    "    data_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                  'marital-status', 'occupation', 'relationship', 'race', \n",
    "                  'sex', 'capital-gain', 'capital-loss', 'hours-per-week', \n",
    "                  'native-country', 'income']\n",
    "\n",
    "    dataframe = pd.read_csv(data_file, names=data_names)\n",
    "    for i in dataframe.columns:\n",
    "        dataframe[i].replace(' ?', np.nan, inplace=True)\n",
    "    dataframe.dropna(inplace=True)\n",
    "    dataframe.drop(['fnlwgt'], axis=1, inplace=True)\n",
    "\n",
    "    # This needs to be run multiple times to capture all categorical data, for some reason\n",
    "    for _ in range(1,5):\n",
    "        for i in range(dataframe.shape[1]):\n",
    "            # check if string data, skip if false\n",
    "            if type(dataframe.iloc[1,i]) == str:\n",
    "            # list uniques in column\n",
    "                unique_classes = list(dataframe.iloc[: , i].unique())\n",
    "            # add binary field for unique with true\n",
    "                for unique_class in unique_classes:            \n",
    "                    dataframe[unique_class.replace('-','').strip()] = dataframe.iloc[:,i] == unique_class\n",
    "                dataframe.drop(axis=1, columns=dataframe.columns[i], inplace=True)\n",
    "\n",
    "    X = np.array(dataframe.drop('>50K',axis=1).values, dtype='float64')\n",
    "    y = np.array(dataframe['>50K'].values, dtype='float64')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0, shuffle=args.shuffle_dataset)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separates data and labels and initialises data loaders\n",
    "def prepare_data(X_train, X_test, y_train, y_test, batch_size, num_workers):\n",
    "\n",
    "    # Initialise train_loaders list for storing each worker's train_loader\n",
    "    train_loaders = list()\n",
    "\n",
    "    # Split training data equally among all workers\n",
    "    remote_X_train = list()\n",
    "    remote_X_train.append(np.array_split(X_train, num_workers))\n",
    "\n",
    "    # Split training labels equally among all workers\n",
    "    remote_y_train = list()    \n",
    "    remote_y_train.append(np.array_split(y_train, num_workers))\n",
    "\n",
    "    # This looks like it's reinitialising these variables as their index 0 values,\n",
    "    # but it's actually discarding an unwanted outer object layer so that we can directly \n",
    "    # reference the data/target lists.\n",
    "    remote_X_train = remote_X_train[0]\n",
    "    remote_y_train = remote_y_train[0]\n",
    "\n",
    "    # Convert training data & labels into torch tensors, then into datasets, then into dataloaders\n",
    "    for i in range(len(remote_X_train)):\n",
    "        remote_X_train[i] = torch.tensor(remote_X_train[i]).float()\n",
    "        remote_y_train[i] = torch.tensor(remote_y_train[i]).float()\n",
    "        train = TensorDataset(remote_X_train[i], remote_y_train[i])\n",
    "        train_loaders.append(DataLoader(train, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "    # Standard test loader setup as we only need one\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    y_test = torch.from_numpy(y_test).float()\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loaders, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a model, optimiser and renyi privacy engine for each worker (only call if DP is needed)\n",
    "def prepare_private_training(device, model, train_loaders, num_workers, batch_size, alphas, lr):\n",
    "    \n",
    "    model_pool = list()\n",
    "    optimizer_pool = list()\n",
    "    priv_eng_pool = list()\n",
    "\n",
    "    # We use deepcopy to make wholly independent copies of the shared model\n",
    "    for _ in range (num_workers):\n",
    "        model_pool.append(copy.deepcopy(model))\n",
    "\n",
    "    # We call the SGD constructor each time to ensure model updates are correctly applied\n",
    "    for model in model_pool:\n",
    "        model.to(device)\n",
    "        opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer_pool.append(opt)\n",
    "\n",
    "    # Attaches privacy engine for each model to each optimiser, effectively replacing\n",
    "    # gradient calculation functions with similar DP-enabled ones.\n",
    "    for i in range(len(model_pool)):    \n",
    "        privacy_engine = PrivacyEngine(\n",
    "                            model_pool[i],\n",
    "                            batch_size=batch_size,\n",
    "                            sample_size=len(train_loaders[i].dataset),\n",
    "                            alphas=alphas,\n",
    "                            noise_multiplier = 1.0,\n",
    "                            max_grad_norm = 1.0)\n",
    "\n",
    "        privacy_engine.attach(optimizer_pool[i])\n",
    "\n",
    "    return model_pool, optimizer_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a model and optimiser for each worker (only call if DP is not needed)\n",
    "def prepare_training(device, model, train_loaders, num_workers, lr):\n",
    "    \n",
    "    model_pool = list()\n",
    "    optimizer_pool = list()\n",
    "\n",
    "    # We use deepcopy to make wholly independent copies of the shared model\n",
    "    for _ in range (num_workers):\n",
    "        model_pool.append(copy.deepcopy(model))\n",
    "\n",
    "    # We call the SGD constructor each time to ensure model updates are correctly applied\n",
    "    for model in model_pool:\n",
    "        model.to(device)\n",
    "        opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer_pool.append(opt)\n",
    "\n",
    "    return model_pool, optimizer_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(104, 20)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    # pass-forward procedure using a relu and a softmax output\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 104)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main model training routine\n",
    "def train(device, model, optimizer, train_loader, epoch, i, disable_verbose_training, disable_dp, delta):\n",
    "    model.train()\n",
    "\n",
    "    # We will store the cross-entropy loss to report it later    \n",
    "    losses = []\n",
    "\n",
    "    # Standard pytorch training loop\n",
    "    for _batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        #print(pred.view(-1,).data)\n",
    "        #print(target.view(-1,).data)\n",
    "        loss = F.binary_cross_entropy(pred.view(-1,), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if not disable_verbose_training:\n",
    "        # If DP is enabled, show our data leakage as we go along with loss output\n",
    "        if not disable_dp:\n",
    "            epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "            print(\n",
    "                    f\"Model ID: {i:2d}   \"\n",
    "                    f\"Train Epoch: {epoch:2d}   \"\n",
    "                    f\"Loss: {np.mean(losses):.6f} \"\n",
    "                    f\"(ε = {epsilon:.2f}, δ = {delta}) for α = {best_alpha}\"\n",
    "            )\n",
    "        else:\n",
    "            # If we are not using DP, just show the loss output \n",
    "            print(f\"Model ID: {i:2d}   Train Epoch: {epoch:2d}   Loss: {np.mean(losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When all workers have trained their local models, bring models together and average them\n",
    "def aggregate_models(device, models):\n",
    "\n",
    "    params = list()\n",
    "    num_workers = len(models)\n",
    "\n",
    "    # Deep copying the model parameters directly makes them easier to work with\n",
    "    for model_id in range(num_workers):\n",
    "        params.append(list(copy.deepcopy(models[model_id]).parameters()))\n",
    "\n",
    "        # Remove outer object layer of each parameter list so we can access it directly\n",
    "        for layer_id in range(len(params[0])):\n",
    "            params[model_id][layer_id] = params[model_id][layer_id].data\n",
    "\n",
    "    agg_params = list()\n",
    "\n",
    "    # Take the mean average of worker parameters at each model layer\n",
    "    for layer_id in range(len(params[0])):\n",
    "\n",
    "        agg_layer = params[0][layer_id]\n",
    "\n",
    "        for worker_id in range(1, num_workers):\n",
    "            agg_layer = agg_layer + params[worker_id][layer_id]\n",
    "\n",
    "        agg_layer = agg_layer / num_workers\n",
    "        agg_params.append(agg_layer)\n",
    "\n",
    "    # Initialise new shared model to be used, and access parameters directly\n",
    "    new_model = Net().to(device)\n",
    "    new_params = list(new_model.parameters())\n",
    "\n",
    "    # Turn off pytorch autograd and overwrite the new model parameters with the averaged params\n",
    "    with torch.no_grad():\n",
    "        for layer_index in range(len(new_params)):\n",
    "            new_params[layer_index].set_(agg_params[layer_index])\n",
    "\n",
    "    # This will be our new shared model for testing and further training\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Test routine\n",
    "def test(data_loader, model):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # Standard pytorch test loop\n",
    "    for data, target in data_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "    # We take the mean of the testing set loss        \n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    # We could print the test loss output or just return it.\n",
    "    # print('Test set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_prepare_training(device, init_model, train_loaders, args):\n",
    "    # Case switch based on whether or not we want to use DP (see the Arguments class)\n",
    "    if not args.disable_dp:\n",
    "        models,opts = prepare_private_training(device, init_model, train_loaders, args.workers, args.batch_size, args.alphas, args.lr)\n",
    "    else:\n",
    "        models,opts = prepare_training(device, init_model, train_loaders, args.workers, args.lr)\n",
    "    \n",
    "    return models,opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime():\n",
    "\n",
    "    # We can enable cuda, but this small model runs faster on CPU\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(\"GPU training enabled\" if use_cuda else \"No GPU detected - training using CPU\")\n",
    "\n",
    "    # Start the performance timer\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # TODO - update code so our net layers can be defined in Net constructor arguments\n",
    "    init_model = Net()\n",
    "\n",
    "    # Load UCI Adult Data Set - downloaded from http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "    X_train, X_test, y_train, y_test = load_adult(args)\n",
    "\n",
    "    # TODO - implement test loaders and test function\n",
    "    train_loaders, _ = prepare_data(X_train, X_test, y_train, y_test, args.batch_size, args.workers)\n",
    "\n",
    "    # List to store our train-time f1 scoring\n",
    "    f1_scores = list()\n",
    "\n",
    "    # Case switch based on whether or not we want to use DP (see the Arguments class)\n",
    "    models,opts = select_prepare_training(device, init_model, train_loaders, args)\n",
    "\n",
    "    # Train all of our worker models on their local data, then combine and aggregate their models\n",
    "    for epoch in range(1,args.epochs+1):\n",
    "        for i in range(len(models)):\n",
    "            train(device, models[i], opts[i], train_loaders[i], epoch, i, args.disable_verbose_training, args.disable_dp, args.delta)\n",
    "        new_model = aggregate_models(device, models)\n",
    "        models,opts = select_prepare_training(device, new_model, train_loaders, args)\n",
    "\n",
    "        # Every 5 epochs record the F1 Score of the aggregated model\n",
    "        if (epoch%5)==0:\n",
    "            with torch.no_grad():\n",
    "                new_model.to(device='cpu')\n",
    "                pred = np.array(new_model(torch.tensor(X_test).float()))\n",
    "                y_pred = pred.reshape(-1,)[:] > 0.5\n",
    "                y_test = y_test.reshape(-1,)[:] > 0.5\n",
    "                f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "                new_model.to(device)\n",
    "\n",
    "    # Stop the performance timer and print\n",
    "    stop = time.perf_counter()\n",
    "    print(f\"Completed in {stop-start:0.4f} seconds\")\n",
    "\n",
    "    # Turn off pytorch autograd to run some metrics \n",
    "    with torch.no_grad():\n",
    "        # Numpy isn't CUDA compatible so bring the model back to the CPU\n",
    "        new_model.to(device='cpu')\n",
    "\n",
    "        # Convert model predictions to Numpy array of Boolean values\n",
    "        pred = np.array(new_model(torch.tensor(X_test).float()))\n",
    "        y_pred = pred.reshape(-1,)[:] > 0.5\n",
    "\n",
    "        # Scikit-learn  metrics - Precision, Recall, F1 score\n",
    "        print(f\"Precision = {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "        print(f\"Recall = {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "        print(f\"F1 score = {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "\n",
    "        # Create a confusion matrix to see how well we perform on Adult income prediction\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Display confusion matrix\n",
    "        plt.matshow(cm)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot graph of F1 Score history over all epochs\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.plot(range(1,args.epochs+1,5),f1_scores)\n",
    "        plt.title('Model F1 Score')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train all models, show how "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
