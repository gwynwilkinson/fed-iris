{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdp import PrivacyEngine\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 1000\n",
    "        self.iterations = 1\n",
    "        self.lr = 0.01\n",
    "        self.shuffle_dataset = True\n",
    "        self.batch_size = 8\n",
    "        self.test_batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "        self.disable_federated = True\n",
    "        self.disable_dp = False\n",
    "        self.alphas=[1.1, 2, 4, 10]\n",
    "        self.delta = 1.0\n",
    "        self.workers = 1\n",
    "        self.disable_verbose_training = True\n",
    "    \n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separates data and labels and initialises data loaders\n",
    "def prepare_data(dataset, shuffle_dataset, batch_size, num_workers):\n",
    "\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "\n",
    "    # Standard train/test split of 0.75/0.25 (see sklearn docs), checks shuffle_dataset flag\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0, shuffle=shuffle_dataset)\n",
    "\n",
    "    # Initialise train_loaders list for storing each worker's train_loader\n",
    "    train_loaders = list()\n",
    "\n",
    "    # Split training data equally among all workers\n",
    "    remote_X_train = list()\n",
    "    remote_X_train.append(np.array_split(X_train, num_workers))\n",
    "\n",
    "    # Split training labels equally among all workers\n",
    "    remote_y_train = list()    \n",
    "    remote_y_train.append(np.array_split(y_train, num_workers))\n",
    "\n",
    "    # This looks like it's reinitialising these variables as their index 0 values,\n",
    "    # but it's actually discarding an unwanted outer object layer so that we can directly \n",
    "    # reference the data/target lists.\n",
    "    remote_X_train = remote_X_train[0]\n",
    "    remote_y_train = remote_y_train[0]\n",
    "\n",
    "    # Convert training data & labels into torch tensors, then into datasets, then into dataloaders\n",
    "    for i in range(len(remote_X_train)):\n",
    "        remote_X_train[i] = torch.tensor(remote_X_train[i]).float()\n",
    "        remote_y_train[i] = torch.tensor(remote_y_train[i]).long()\n",
    "        train = TensorDataset(remote_X_train[i], remote_y_train[i])\n",
    "        train_loaders.append(DataLoader(train, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "    # Standard test loader setup as we only need one\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    y_test = torch.from_numpy(y_test).long()\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loaders, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a model, optimiser and renyi privacy engine for each worker (only call if DP is needed)\n",
    "def prepare_private_training(model, train_loaders, num_workers, batch_size, alphas, lr):\n",
    "    \n",
    "    model_pool = list()\n",
    "    optimizer_pool = list()\n",
    "    priv_eng_pool = list()\n",
    "\n",
    "    # We use deepcopy to make wholly independent copies of the shared model\n",
    "    for _ in range (num_workers):\n",
    "        model_pool.append(copy.deepcopy(model))\n",
    "\n",
    "    # We call the SGD constructor each time to ensure model updates are correctly applied\n",
    "    for model in model_pool:\n",
    "        opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer_pool.append(opt)\n",
    "\n",
    "    # Attaches privacy engine for each model to each optimiser, effectively replacing\n",
    "    # gradient calculation functions with similar DP-enabled ones.\n",
    "    for i in range(len(model_pool)):    \n",
    "        privacy_engine = PrivacyEngine(\n",
    "                            model_pool[i],\n",
    "                            batch_size=batch_size,\n",
    "                            sample_size=len(train_loaders[i].dataset),\n",
    "                            alphas=alphas,\n",
    "                            noise_multiplier = 1.0,\n",
    "                            max_grad_norm = 1.0)\n",
    "\n",
    "        privacy_engine.attach(optimizer_pool[i])\n",
    "\n",
    "    return model_pool, optimizer_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a model and optimiser for each worker (only call if DP is not needed)\n",
    "def prepare_training(model, train_loaders, num_workers, lr):\n",
    "    \n",
    "    model_pool = list()\n",
    "    optimizer_pool = list()\n",
    "\n",
    "    # We use deepcopy to make wholly independent copies of the shared model\n",
    "    for _ in range (num_workers):\n",
    "        model_pool.append(copy.deepcopy(model))\n",
    "\n",
    "    # We call the SGD constructor each time to ensure model updates are correctly applied\n",
    "    for model in model_pool:\n",
    "        opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        optimizer_pool.append(opt)\n",
    "\n",
    "    return model_pool, optimizer_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)\n",
    "        self.fc2 = nn.Linear(8, 3)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "    # pass-forward procedure using a relu and a softmax output\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main model training routine\n",
    "def train(model, optimizer, train_loader, epoch, i, disable_verbose_training, disable_dp, delta):\n",
    "    model.train()\n",
    "\n",
    "    # We will store the cross-entropy loss to report it later    \n",
    "    losses = []\n",
    "\n",
    "    # Standard pytorch training loop\n",
    "    for _batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = F.cross_entropy(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if not disable_verbose_training:\n",
    "        # If DP is enabled, show our data leakage as we go along with loss output\n",
    "        if not disable_dp:\n",
    "            epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "            print(\n",
    "                    f\"Model ID: {i:2d}   \"\n",
    "                    f\"Train Epoch: {epoch:2d}   \"\n",
    "                    f\"Loss: {np.mean(losses):.6f} \"\n",
    "                    f\"(ε = {epsilon:.2f}, δ = {delta}) for α = {best_alpha}\"\n",
    "            )\n",
    "        else:\n",
    "            # If we are not using DP, just show the loss output \n",
    "            print(f\"Model ID: {i:2d}   Train Epoch: {epoch:2d}   Loss: {np.mean(losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When all workers have trained their local models, bring models together and average them\n",
    "def aggregate_models(models):\n",
    "\n",
    "    params = list()\n",
    "    num_workers = len(models)\n",
    "\n",
    "    # Deep copying the model parameters directly makes them easier to work with\n",
    "    for model_id in range(num_workers):\n",
    "        params.append(list(copy.deepcopy(models[model_id]).parameters()))\n",
    "\n",
    "        # Remove outer object layer of each parameter list so we can access it directly\n",
    "        for layer_id in range(len(params[0])):\n",
    "            params[model_id][layer_id] = params[model_id][layer_id].data\n",
    "\n",
    "    agg_params = list()\n",
    "\n",
    "    # Take the mean average of worker parameters at each model layer\n",
    "    for layer_id in range(len(params[0])):\n",
    "\n",
    "        agg_layer = params[0][layer_id]\n",
    "\n",
    "        for worker_id in range(1, num_workers):\n",
    "            agg_layer = agg_layer + params[worker_id][layer_id]\n",
    "\n",
    "        agg_layer = agg_layer / num_workers\n",
    "        agg_params.append(agg_layer)\n",
    "\n",
    "    # Initialise new shared model to be used, and access parameters directly\n",
    "    new_model = Net()\n",
    "    new_params = list(new_model.parameters())\n",
    "\n",
    "    # Turn off pytorch autograd and overwrite the new model parameters with the averaged params\n",
    "    with torch.no_grad():\n",
    "        for layer_index in range(len(new_params)):\n",
    "            new_params[layer_index].set_(agg_params[layer_index])\n",
    "\n",
    "    # This will be our new shared model for testing and further training\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Test routine\n",
    "def test(data_loader, model):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # Standard pytorch test loop\n",
    "    for data, target in data_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "    # We take the mean of the testing set loss        \n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    # We could print the test loss output or just return it.\n",
    "    # print('Test set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_prepare_training(init_model, train_loaders):\n",
    "    # Case switch based on whether or not we want to use DP (see the Arguments class)\n",
    "    if not args.disable_dp:\n",
    "        models,opts = prepare_private_training(init_model, train_loaders, args.workers, args.batch_size, args.alphas, args.lr)\n",
    "    else:\n",
    "        models,opts = prepare_training(init_model, train_loaders, args.workers)\n",
    "    \n",
    "    return models,opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8947368421052632\n",
      "Recall = 0.8947368421052632\n",
      "F1 score = 0.8947368421052632\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing demo function to call all above functions and produce a federated learning model.\n",
    "init_model = Net()\n",
    "\n",
    "# Here we're using the Iris dataset - TODO change this for something more challenging\n",
    "iris = load_iris()\n",
    "train_loaders, test_loader = prepare_data(iris, args.shuffle_dataset, args.batch_size, args.workers)\n",
    "\n",
    "# Case switch based on whether or not we want to use DP (see the Arguments class)\n",
    "models,opts = select_prepare_training(init_model, train_loaders)\n",
    "\n",
    "# Train all of our worker models on their local data, then combine and aggregate their models\n",
    "for epoch in range(1,args.epochs+1):\n",
    "    for i in range(len(models)):\n",
    "        train(models[i], opts[i], train_loaders[i], epoch, i, args.disable_verbose_training, args.disable_dp, args.delta)\n",
    "    new_model = aggregate_models(models)\n",
    "    models,opts = select_prepare_training(new_model, train_loaders)\n",
    "\n",
    "# Turn off pytorch autograd to run some scikit learn metrics (Precision, Recall, F1 score)\n",
    "with torch.no_grad():\n",
    "    _, X_test, _, y_test = train_test_split(iris.data, iris.target, random_state=0, shuffle=args.shuffle_dataset)\n",
    "    y_pred = new_model(torch.tensor(X_test).float())\n",
    "    y_pred = np.array(y_pred.argmax(1))\n",
    "    print(f\"Precision = {precision_score(y_test, y_pred, average='micro')}\")\n",
    "    print(f\"Recall = {recall_score(y_test, y_pred, average='micro')}\")\n",
    "    print(f\"F1 score = {f1_score(y_test, y_pred, average='micro')}\")\n",
    "\n",
    "    # Create a confusion matrix to see how well we perform on Iris\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Create new matplotlib plot to display confusion matrix\n",
    "    plt.matshow(cm)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
